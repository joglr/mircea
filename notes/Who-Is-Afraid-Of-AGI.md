# Who Is Afraid of AGI?

*February, 2023*


TL;DR; version:

![](../docs/assets/this_is_jim.png)
---

AGI stands for Artificial General Intelligence. 

It is theoretized to be the level of AI at which an "algorithm" exposes general inteligence on par with a human. It is the "phylosopher's stone" of the modern techno-alchemist. 

Many people, and sometimes even smart ones, are afraid that we are on the verge of inventing an AGI that will be more "intelligent" (whatever that may mean) than humans. Reportedly, Elon Musk can't sleep at night because of his worries about AI and AGI. 

But why be afraid of the AGI? 

Well, because at some point the AGI will start improving itself and will turn itself into a super-AGI which will be so much more intelligent than humans that it will be like a God compared to us. And then, it might just decide to dispose with our species. Or make us into its pets. The techno-apocalypse, also called the "singularity", was an idea proposed by Ray Kurzweil. It has these days become popular under the fear of the AGI, probably because of the popularity of the recent advances in AI and the associated hopes (i.e., if it can generate text that looks like thinking, maybe it can think...).  

Note that the field of AI is nowhere near to any such level of generality: most of the algorithms are narrowly focused on a single domain: playing chess, playing go, generating text, classifying images, etc. (And it should be like that. Algorithms are tools, and tools are there to enhance our capabilities, not to replace us). 

Moreover, my advice those who are afraid of AI turning into AGI  is to realize that AI is just advanced mathematics. It's a mathematical function. You give it some input. It provides some output.  Surely, there is a clear increase in complexity when one goes from linear regression, to logistic regression, to neural networks, to deep neural networks, and to large language models (e.g. ChatGPT), but in the end, each one of these models is, nothing more than a very complicated mathematical function representing a statistical approximation derived from very large quantities of example data. A mathematical function that you don't understand is still a ... mathematical function. 

So, to wrap it up, it is my argument here that by being afraid of AI, one is literally afraid of mathematics. Yes, it's true, we're all a bit afraid of mathematics, but that's because we're too lazy to put the time and understand it, not because of any objective reason. 

In fact, until recently, those who were afraid of mathematics would rather not mention it. It is really a new activity to express your fears about it, and it is definitely a silly idea to [try to stop everybody else from doing mathematics for six months](https://www.npr.org/2023/03/29/1166891536/an-open-letter-signed-by-tech-leaders-researchers-proposes-delaying-ai-developme).

Anyway, to make a long story short, my advice is that you don't lose sleep over fears of AGI-apocalypse, and definitely don't keep your partner awake with your worries either: they might just leave you and then you'll [be sad](https://radaronline.com/p/elon-musk-sad-boy-phase-breakup-grimes-lonely-dog/) 


## Further Notes

- Unlikely to have AGI w/o a sense of self [eliot miranda on linkedin](https://www.linkedin.com/feed/update/urn:li:activity:7022617377229983744/) . And a sense of self is not well-defined without a body is my conjecture. 
- Elon musk [can't sleep at night](https://www.geospatialworld.net/blogs/scares-elon-musk-artificial-intelligence/) because he's afraid of AI; Bostrom is at least capitalizing on his fears by [writing a book on the topic](https://www.vox.com/future-perfect/2018/11/2/18053418/elon-musk-artificial-intelligence-google-deepmind-openai)
- The whole discussion about LLMs and the fear thereof, reminds of of the Arthur C. Clarke saying "*Any sufficiently advanced technology is indistinguishable from magic*". And if something is magic, you have all the rights to be afraid of it, I guess.
- There's probably a small cohort of "fearful" -- the behavior of which can be explained by another dictum: "*You can not make somebody understand something, if their job depends on not understanding it*". These are the pundits, the journalists, the researchers who by hyping up the "potential" of the technology benefit themselves in one way or another. 
- There's also the fear of some of these algorithms taking over our jobs. It might take over some jobs and it will help us get better at other jobs. Washing dishes has been taken over by robots and nobody misses it. Chess has been automated to the point where no human can beat an algorithm, and that does not prevent us to still enjoy the game, admire the amazingly talented human players including Magnus Carlsen (that we rewarded with a net worth of 50M). In fact, the chess engines are just helping everybody to become better at playing the game. This second is the future that I see of language models, e.g. we'll all get better at writing, we won't be "replaced by a writing" algorithm, whatever that may mean. 
- Sure, if we put statstical functions that we don't fully understand inside systems that can harm humans, than we should be afraid. But that's a stupid thing to do in the first place; and in that case the fear should not be of the AI component itself but of the unpredictibility of the whole resulting system. My rule of thumb is this: use AI if you can "undo" its action (spam classification, code completion) or if you don't give a speck about mistakes (image search that returns a muffin in between dogs) then use AI as much as you can. However, don't let an AI algorithm drive your car or shoot weapons. Driving over a child can't be undone . And you do care about mistsakes in weapon-related situations too. 