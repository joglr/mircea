# Who Is Afraid of AGI?

or, *Why you shouldn't be afraid of mathematics*

*February, 2023*


TL;DR; version:

![](../docs/assets/this_is_jim.png)
---

AGI stands for Artificial General Intelligence. 

It is theoretized to be the level of AI at which an "algorithm" exposes general inteligence on par with a human. It is the "phylosopher's stone" of the modern techno-alchemist. 

Many people, and sometimes even smart ones, are afraid that we are on the verge of inventing an AGI that will be more "intelligent" (whatever that may mean) than humans. Reportedly, Elon Musk can't sleep at night because of his worries about AI and AGI. 

But why be afraid of the AGI? 

Well, because that "more intelligent" than humans AGI will start improving itself and will turn itself into a super-AGI which will be so much more intelligent than all of us that it will be like a God compared to us. And then, it might just decide to dispose with our species. Or make us into its pets. The techno-apocalypse, also called the "singularity", was an idea originally proposed by Ray Kurzweil, and it seems to not let some people sleep at night. The fear of AI has flared these days, probably because of the popularity of the recent advances in AI and the associated hopes and journalistic and marketing amplifications.  

Note that the field of AI is nowhere near to any such level of generality: most of the algorithms are narrowly focused on a single domain: playing chess, playing go, generating computer levels, generating text, classifying images, etc. (And it should be like that. Algorithms are tools, and tools are there to enhance our capabilities, not to replace us). 

Moreover, my advice those who are afraid of AI is to realize that AI is just advanced mathematics. Even the impressive ChatGPT can be seen in some sense as a giant mathematical function that takes as input the last 1000 tokens of text, and generates text. You give it some input. It provides some output.  Surely, there is a clear increase in complexity when one goes from linear regression, to logistic regression, to neural networks, to deep neural networks, and to large language models (e.g. ChatGPT), but in the end, each one of these models is, nothing more than a very complicated mathematical function defined based on some statistical approximation derived from very large quantities of example data. A mathematical function that you don't understand is still a ... mathematical function. 

By being afraid of AI, one is literally afraid of mathematics. Yes, it's true, we're all a bit afraid of mathematics, but that's because we're too lazy to put the time and understand it, not because of any objective reason. 

In fact, until recently, those who were afraid of mathematics would rather not mention it. It is really a new activity to express your fears about it, and it is definitely a silly idea to [try to stop everybody else from doing mathematics for six months](https://www.npr.org/2023/03/29/1166891536/an-open-letter-signed-by-tech-leaders-researchers-proposes-delaying-ai-developme). 

Thus, don't lose sleep over fears of AGI-apocalypse: God is not a mathematical function computed in the Sillicon Valley.

And instead of proposing that people stop doing research in AI, I think we'd all be better served by a ban on people who do not work in AI (e.g. journalists, marketing, pundits, Elon Musk): for six months they should not be allowed to worry in public about the techno-apocalypse. It'd be a better, quieter, more rational world. 

## Further Notes

- Unlikely to have AGI w/o a sense of self [eliot miranda on linkedin](https://www.linkedin.com/feed/update/urn:li:activity:7022617377229983744/) . And a sense of self is not well-defined without a body is my conjecture. 
- Elon musk [can't sleep at night](https://www.geospatialworld.net/blogs/scares-elon-musk-artificial-intelligence/) because he's afraid of AI; Bostrom is at least capitalizing on his fears by [writing a book on the topic](https://www.vox.com/future-perfect/2018/11/2/18053418/elon-musk-artificial-intelligence-google-deepmind-openai)
- The whole discussion about LLMs and the fear thereof, reminds of of the Arthur C. Clarke saying "*Any sufficiently advanced technology is indistinguishable from magic*". And if something is magic, you have all the rights to be afraid of it, I guess.
- There's probably a small cohort of "fearful" -- the behavior of which can be explained by another dictum: "*You can not make somebody understand something, if their job depends on not understanding it*". These are the pundits, the journalists, the researchers who by hyping up the "potential" of the technology benefit themselves in one way or another. 
- There's also the fear of some of these algorithms taking over our jobs. It might take over some jobs and it will help us get better at other jobs. Washing dishes has been taken over by robots and nobody misses it. Chess has been automated to the point where no human can beat an algorithm, and that does not prevent us to still enjoy the game, admire the amazingly talented human players including Magnus Carlsen (that we rewarded with a net worth of 50M). In fact, the chess engines are just helping everybody to become better at playing the game. This second is the future that I see of language models, e.g. we'll all get better at writing, we won't be "replaced by a writing" algorithm, whatever that may mean. 
- Sure, if we put statstical functions that we don't fully understand inside systems that can harm humans, than we should be afraid. But that's a stupid thing to do in the first place; and in that case the fear should not be of the AI component itself but of the unpredictibility of the whole resulting system. My rule of thumb is this: use AI if you can "undo" its action (spam classification, code completion) or if you don't give a speck about mistakes (image search that returns a muffin in between dogs) then use AI as much as you can. However, don't let an AI algorithm drive your car or shoot weapons. Driving over a child can't be undone . And you do care about mistsakes in weapon-related situations too. 