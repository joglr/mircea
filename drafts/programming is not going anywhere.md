Alternative subtitle: "what is missing for prompting-foundational models to replace programming". This would also be a positive spin on the paper. 



This paper is a thought experiment triggered by the general enthusiasm and high hopes related to generative AI. In particular, this is a critique of a particular claim related to Gen AI: that a tool like an LLM, or a foundational, model will at some point (usually "soon") replace programming completely. The reasoning is the simple: 
- in the early days of computing, developers used to write assembly language
- then compilers came and allowed the programmers to write high-level languages and the assembly language was *generated*, thus eliminating the need for programmers to write assembly
- LLMs are going to take this generation to the next level, by automatically eliminating the need for programmers to write code in high-level languages

The heavy lifting of the syllogism is done by the overloading of the term *generate* with two completely different meanings:
- in the compiler case, it means code transformation
- in the LLM case it means "statistically generation" of linguistic tokens


## Software Evolution to the Rescue 

The main arguments of this paper are those stemming from software maintenance and evolution. There are other people who have written about the inherent limitations of the LLMs when it comes to trutfulness, and etc. 

We are software engineers, and are writing from a position of understanding the challenges that software evolution poses to programming. 





> Programming languages are the easiest way in which one can specify systems; if there was an easier way, we would have used it. (to find the author)


The method we use in this paper is a principled critique. It is not an empirical study; there are many such studies, but the problem with them is that their replicability is difficult: a study that uses ChatGPT or CoPilot can't be replicated once new versions of these systems are released. The results would probably be different. 

However, we need here and there to take into account knowledge from NLP to explain some of the characteristics of the LLM part. 



## But we are not writing assembly anymore ... 

The first argument is that just as we stopped writing assembly because of the coming of the high-level languages, in the same way, LLMs will replace programming. 

There is a big difference between the HL languages and LLMs: one is a language the other is a ... language model. This has impacts in evolution.

## Bricolage: constructing a system from "wishes"

The strong argument of the most hopeful software engineers is the following: as the AI models are getting better, we will be able to write systems just by writing in natural language what we want. 

Let us assume that one can do this and that it works. 

The main difference between natural language and a programming language is that one is precise and one is ambiguous. The ambiguity, be definition means the following: 

### for any wish there can be many programs to satisfy it

- this is also visible in the UI of LLMs these days, as one types a *prompt*, the system returns an answer, but together with the answer one can use a "give me more" button. 

wish(i) => P{i}...P{j} 

This is not surprising, since it is inherent in the architecture of the LLM - every word generated impacts the next words that follow it. Thus, generating a different word 


### there is no guarantee that the program selected for implementing a wish is correct

This might be the case for simple programs.
And might be the case for the current version.
But one can never be sure that 

### There is no guarantee that a different LLM or a different version of an LLM will generate the same programs in the same order. This has significant implications for evolution

wish(i, v(LLM) = P{i,v(LLM)} ... P{n,v(LLM)}

Thus, if one would construct a bricolage of a system by composing together programs generated by wishes, when it comes the time for evolving them, they would have two choices: 

1. maintain the program now generated in the programming language used; but this is "supposed to not be necessary anymore"
2. given that their system was created by the sequence of wishes 1...n, either 
	- replacing the wish(i) that is the one that needs to be updated and re-run the other wishes unchanged - however, this is not possible because there is no guarantee that a new version of the LLM will 
	- adding another wish asking the LLM to modify the system. this requires that the LLM has to be fed the whole system as a whole as part of the training. so instead of replacing an older wish, one will just amend it. imprecise definitions of program transformations might be possible, but how is the programmer to verify that their imprecise request is fulfilled? in our thought experiment, all they write are "wishes" but to know which of the wishes really are what they want they can't check the generated code. so they are left with the only way to test is "trying out the system" or writing "tests" that ensure that the generated code 



### The higher the abstraction level of the "wish", the more possible programs will be generated for it

For simple wishes, there are few possible programs that the LLM will generate. E.g. `provide a fibonacci implementation` . There are few implementations, and all of them have been written a billion times on the internet. 

On the other hand, asking high-level wishes, is fraught with danger. If one asks "generate for me a todo app" there could be potentially thousands of such implementations. This assuming that the system would 



### "Wish Review" will be challenging

Assuming that for large systems, one would want to review the wishes, it will be fascinating to see discussions of the form:
- your wish W1 


### This system is not sustainable

The LLM works based on examples it finds on the internet. Thus, it can not work on new technologies and languages until they have a critical mass. 


#### The low level wishes are exactly those for which there are better solutions

Take for example the wish *"write a function that detects whether the code runs in a browser"*. The LLM might come back with the following implementation: 

```js
function isMobile() {
	return navigator.userAgent.match (r"Android|iOS")
}
```

The problem with this code is that it is not maintainable. It might work for today, but when new user agents appear, it will be stuck in the past. Compare this with packages, like in `npm`. There, one depending on a package, one gets for free updates. 

The defenders of no-more-programming-needed would probably argue that a new version of the LLM would generate an updated version of this function. However, until the LLM finds sufficient examples of the new code to learn the new pattern, the application that uses it will be broken. Compare this with the alternative:
- the maintainer of the little `isMobile` package finds out that there's a new user agent, updates the regex, pushes an update
- the next morning, all the users of the package get the update

Compare with the delay required by the LLM to update:
- several months required for the new pattern to appear on the net
- several more months for training

Sure, but what if the LLM would generate code that uses the isMobile package. The delay is the same. The isMobile package might have been the most popular at the time of training, but there might be a better implementation that appeared in the meantime. This is again going to result in many months of delay. If the package that is 'better' is just better at detecting mobile agents, the problem is not that big. However, if the package is better because it fixes vulnerabilities, then this is a problem. 

Sure, one would argue, why not use semantic versioning in the source code? Yes, that could be the case, but then the developers have to look again at the code. No. They only have to let the LLM know that they want a solution with another package than the one that has a vulnerability. But then delay. Ok, they can modify it by hand. Sure, but... they are programming now. And programming was supposed to be not necessary anymore. 

Well, it's a small exception. Updating little snippets that use libraries is not too much of a trouble. Indeed. However, not all dependencies are of the same caliber as `isMobile`. Others are cross-cutting through the whole system. 

Ok, you're just not getting it. People will not have to look under the hood. The code generated by the LLM will be as good as isMobile. So packages are going away. 

Yes... Nope. That is not a better system. 



### So if the abstract wishes are hared to maintain, and the low-level wishes have better alternatives, what is left?

One thing that is left is boilerplate. 
And detecting latent patterns in the code.

Indeed, the LLMs will eventually, very likely converge into being the next thing after auto-completion. Indeed, they can do more than 




## So What Are these systems good for then?

Let us remember what they are. 
They are models of language. 
They predict the most likely next token.
So in the measure in which one tries to write the "average" program that has been written before many times, they might be able to use the LLM. 
It is unlikely that 







### Data-driven haphazard model-driven engineering

- is there something to learn from model-driven engineering?
- the limitations of code generated 